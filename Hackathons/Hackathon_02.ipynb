{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp23/blob/main/Hackathons/Hackathon_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3DtjEbf8M4I"
      },
      "source": [
        "**Introduction**\n",
        "\n",
        "Machine Learning algorithms have become an increasingly important tool for analyzing the data from the Large Hadron Collider (LHC). Identification of particles in LHC collisions is an important task of LHC detector reconstruction algorithms.\n",
        "\n",
        "Here we present a challenge where one of the detectors (the Electromagnetic Calorimeter or ECAL) is used as a camera to analyze detector images from two types of particles: electrons and photons that deposit their energy in this detector.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Each pixel in the image corresponds to a detector cell, while the intensity of the pixel corresponds to how much energy is measured in that cell. Timing of the energy deposits are also available, though this may or may not be relevant. The dataset contains 32x32 Images of the energy hits and their timing (channel 1: hit energy and channel 2: its timing) in each calorimeter cell (one cell = one pixel) for the two classes of particles: Electrons and Photons. The dataset contains around four hundred thousand images for electrons and photons. Please note that your final model will be evaluated on an unseen test dataset.\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "Please use a Machine Learning model of your choice to achieve the highest possible classification performance on the provided dataset. Please provide a Jupyter Notebook that shows your solution.\n",
        "\n",
        "Evaluation Metrics\n",
        "ROC curve (Receiver Operating Characteristic curve) and AUC score (Area Under the ROC Curve)\n",
        "Training and Validation Accuracy\n",
        "The model performance will be tested on a hidden test dataset based on the above metrics.\n",
        "\n",
        "**Deliverables**\n",
        "\n",
        "Google Colab Jupyter Notebook showing your solution along with the final model accuracy (Training and Validation), ROC curve and AUC score. More details regarding the format of the notebook can be found in the sample Google Colab notebook provided for this challenge.\n",
        "The final trained model including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. ). You are free to choose Machine Learning Framework of your choice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0b6SRpWl2Xh"
      },
      "source": [
        "## Create the appropriate project folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYRULwKaw_A6"
      },
      "outputs": [],
      "source": [
        "mkdir Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at-4-Xub8DYW"
      },
      "outputs": [],
      "source": [
        "cd Particle_Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQGKM10j2CiQ"
      },
      "outputs": [],
      "source": [
        "mkdir data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEtRyfNv9XVn"
      },
      "source": [
        "# Download the Dataset\n",
        "This will download 83MB for SingleElectron and 76MB for SinglePhoton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcK1wY4Qt_7Y"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "!wget https://cernbox.cern.ch/index.php/s/sHjzCNFTFxutYCj/download -O data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\n",
        "!wget https://cernbox.cern.ch/index.php/s/69nGEZjOy3xGxBq/download -O data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BepRE7pn8Du7"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnLzC5paz0hb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "import h5py\n",
        "from keras.models import Sequential\n",
        "from keras.initializers import TruncatedNormal\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBsy7x5O8cWH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_MoJwZ8K6l"
      },
      "source": [
        "# Keras Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzF8AHl_4yUA"
      },
      "outputs": [],
      "source": [
        "lr_init     = 1.e-3    # Initial learning rate  \n",
        "batch_size  = 64       # Training batch size\n",
        "train_size  = 1024     # Training size, max is 160000\n",
        "valid_size  = 1024     # Validation size, max is 44800\n",
        "test_size   = 1024     # Test size\n",
        "epochs      = 20       # Number of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jX_l-WmplJx"
      },
      "source": [
        "It is recommended to use GPU for training and inference if possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwc56kXJ8TLo"
      },
      "source": [
        "# Load Image Data\n",
        "- Two classes of particles: electrons and photons\n",
        "- 32x32 matrices (two channels - hit energy and time) for the two classes of particles electrons and photons impinging on a calorimeter (one calorimetric cell = one pixel).\n",
        "- Note that although timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr4QIMlt424u"
      },
      "outputs": [],
      "source": [
        "img_rows, img_cols, nb_channels = 32, 32, 2        \n",
        "input_dir = 'data'\n",
        "decays = ['SinglePhotonPt50_IMGCROPS_n249k_RHv1', 'SingleElectronPt50_IMGCROPS_n249k_RHv1']\n",
        "\n",
        "def load_data(decays, start, stop):\n",
        "    global input_dir\n",
        "    dsets = [h5py.File('%s/%s.hdf5'%(input_dir,decay)) for decay in decays]\n",
        "    X = np.concatenate([dset['/X'][start:stop] for dset in dsets])\n",
        "    y = np.concatenate([dset['/y'][start:stop] for dset in dsets])\n",
        "    assert len(X) == len(y)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JpHCOf38fDL"
      },
      "source": [
        "# Configure Training / Validation / Test Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RTXS58x46Fq"
      },
      "outputs": [],
      "source": [
        "# Set range of training set\n",
        "train_start, train_stop = 0, train_size\n",
        "assert train_stop > train_start\n",
        "assert (len(decays)*train_size) % batch_size == 0\n",
        "X_train, y_train = load_data(decays,train_start,train_stop)\n",
        "\n",
        "# Set range of validation set\n",
        "valid_start, valid_stop = 160000, 160000+valid_size\n",
        "assert valid_stop  >  valid_start\n",
        "assert valid_start >= train_stop\n",
        "X_valid, y_valid = load_data(decays,valid_start,valid_stop)\n",
        "\n",
        "# Set range of test set\n",
        "test_start, test_stop = 204800, 204800+test_size\n",
        "assert test_stop  >  test_start\n",
        "assert test_start >= valid_stop\n",
        "X_test, y_test = load_data(decays,test_start,test_stop)\n",
        "\n",
        "samples_requested = len(decays) * (train_size + valid_size + test_size)\n",
        "samples_available = len(y_train) + len(y_valid) + len(y_test)\n",
        "assert samples_requested == samples_available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTFIMRnL8w41"
      },
      "source": [
        "# Plot sample of training images\n",
        "Note that although the timing channel is provided, it may not necessarily help the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_IDs16U52-C"
      },
      "outputs": [],
      "source": [
        "plt.figure(1)\n",
        "\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[1,:,:,0])\n",
        "plt.title(\"Channel 0: Energy\")  # Energy\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[1,:,:,1])\n",
        "plt.title(\"Channel 1: Time\")  # Time\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke_NQLiz83jZ"
      },
      "source": [
        "# Define CNN Model\n",
        "This is a sample model. You can experiment with the model and try various architectures and other models to achieve the highest possible performance.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlIJuxXG7KDk"
      },
      "outputs": [],
      "source": [
        "### Define Convolutional Neural Network (CNN) Model ###\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal', input_shape=(img_rows, img_cols, nb_channels)))\n",
        "model.add(Conv2D(16, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n",
        "model.add(Conv2D(32, activation='relu', kernel_size=3, padding='same', kernel_initializer='TruncatedNormal'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='TruncatedNormal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='TruncatedNormal'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid', kernel_initializer='TruncatedNormal'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsQWFD0M86pA"
      },
      "source": [
        "## Train the Model\n",
        "You may further optimize the model, tune hyper-parameters, etc. accordingly to achieve the best performance possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGuyM25L7uc0"
      },
      "outputs": [],
      "source": [
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1.e-6)\n",
        "history=model.fit(X_train, y_train,\\\n",
        "        batch_size=batch_size,\\\n",
        "        epochs=epochs,\\\n",
        "        validation_data=(X_valid, y_valid),\\\n",
        "        callbacks=[reduce_lr],\\\n",
        "        verbose=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLoN43278_-j"
      },
      "source": [
        "## Evaluate the Model  \n",
        "Along with the model accuracy, the prefered metric for evaluation is ROC (Receiver Operating Characteristic) curve and the AUC score (Area under the ROC Curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekuQLYas7xh5"
      },
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "score = model.evaluate(X_valid, y_valid, verbose=1)\n",
        "print('\\nValidation loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "y_pred = model.predict(X_valid)\n",
        "fpr, tpr, _ = roc_curve(y_valid, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Validation ROC AUC:', roc_auc)\n",
        "\n",
        "# Evaluate on test set\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('\\nTest loss / accuracy: %0.4f / %0.4f'%(score[0], score[1]))\n",
        "y_pred = model.predict(X_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Test ROC AUC:', roc_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmSRYI0R72Wn"
      },
      "outputs": [],
      "source": [
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "#plt.legend(loc=2, prop={'size': 15})\n",
        "plt.plot(fpr, tpr, label='Model 1 (ROC-AUC = {:.3f})'.format(roc_auc))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nna7GkM45Y5w"
      },
      "source": [
        "# Submission format: \n",
        "Submit the Google Colab Jupyter Notebook demonstrating your solution in the similar format as illustrated in this notebook. It should contain:\n",
        "*   The final model architecture, parameters and hyper-parameters yielding the best possible performance,\n",
        "*   Its Training and Validation accuracy, \n",
        "*   ROC curve and the AUC score as shown above.\n",
        "*   Also, please submit the final trained model containing the model architecture and its trained weights along with this notebook (For example: HDF5 file, .pb file, .pt file, etc.). Either in this notebook or in a separate notebook show how to load and use your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56_teGfk8cWL"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}