{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp23/blob/main/Exercises/07_deep_computer_vision_with_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Computer Vision with Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "OEWaY_SPoHjj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8lxARkCnyXL"
      },
      "source": [
        "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcOmL14unyXN"
      },
      "source": [
        "❗️ **This will be very slow, unless you are using a GPU**\n",
        "\n",
        "❗️ **If you do not, then you should run this notebook in Colab, using a GPU runtime**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoHybjaRnyXO"
      },
      "source": [
        "Due date: 2023-03-27\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and Linus <br> Torvalds it would be <br>\n",
        "\"07_Exercise7_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br>\n",
        "possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended** <br> \n",
        "**for you to try to understand every line of code.** <br>\n",
        "**Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YF429hZrnyXP"
      },
      "outputs": [],
      "source": [
        "group_name = \"\"\n",
        "names = [\"a\", \"b\", \"c\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_18l9BDanyXQ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxEXr8InnyXQ"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uccX_HkKnyXQ"
      },
      "outputs": [],
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image, interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's **import some data** to see how convolutional filters work. One is a scenic <br>\n",
        "image of china and the other is an image of a flower. The first thing we should <br>\n",
        "do is **normalize the pixels**."
      ],
      "metadata": {
        "id": "SeolOU5zoawQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvhwaonnyXR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "# Load sample images\n",
        "china = load_sample_image(\"china.jpg\") / 255\n",
        "flower = load_sample_image(\"flower.jpg\") / 255\n",
        "images = np.array([china, flower])\n",
        "batch_size, height, width, channels = images.shape\n",
        "print(batch_size, height, width, channels)\n",
        "\n",
        "plt.imshow(china)\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()\n",
        "plt.imshow(flower)\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let's create some filters. Here we're creating image filters that have the <br>\n",
        "shape **7x7x3x2**. So that's a 7x7 grid which will pass over three color channels <br>\n",
        "and we have two filters for each of those dimensions. Here we want two filters <br>\n",
        "as the final dimension to demonstrate creating **vertical** and **horizontal** filters."
      ],
      "metadata": {
        "id": "S9ZSIHLoq2EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 filters\n",
        "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
        "filters[:, 3, :, 0] = 1  # vertical line\n",
        "filters[3, :, :, 1] = 1  # horizontal line\n",
        "plt.imshow(filters[:,:,:,0])\n",
        "plt.show()\n",
        "plt.imshow(filters[:,:,:,1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ur47L9mapY2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that when we look at the shape of the outputs of our filters it now has <br>\n",
        "**final dimension 2 instead of 3**. What we've done here is **reduced our 3** red, <br>\n",
        "green, blue **(RGB) channels to two filter channels** that have picked out the <br>\n",
        "vertical and horizontal lines in all three color channels then added them up."
      ],
      "metadata": {
        "id": "FrNJ2acou-pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
        "print(outputs.shape)\n",
        "\n",
        "plt.imshow(outputs[0, :, :, 1], cmap='gray') # plot 1st image's 2nd feature map\n",
        "plt.axis(\"off\") # Not shown in the book\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cP5LjEVTu2Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_pCaX9pnyXR"
      },
      "outputs": [],
      "source": [
        "def crop(images):\n",
        "    return images[150:220, 130:250]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our different color channels to see what an unfiltered image <br>\n",
        "looks like."
      ],
      "metadata": {
        "id": "e3lnBVYjvXBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA6Yb4lcnyXR"
      },
      "outputs": [],
      "source": [
        "plot_image(crop(images[0, :, :, 0]))\n",
        "plt.show()\n",
        "plot_image(crop(images[0, :, :, 1]))\n",
        "plt.show()\n",
        "plot_image(crop(images[0, :, :, 2]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjSPlh8rnyXS"
      },
      "source": [
        "# Basics: Filters and Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIS0_bPTnyXS"
      },
      "source": [
        "## Task 1: Filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c1L_PGInyXS"
      },
      "outputs": [],
      "source": [
        "for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n",
        "    plot_image(crop(outputs[0, :, :, feature_map_index]))\n",
        "    plt.title(filename)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaUjUD9bnyXS"
      },
      "source": [
        "**Task 1 a)**: Describe how the filters work and what their purpose in a CNN is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnymzLuLvm2"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OklYadm7nyXT"
      },
      "source": [
        "**Task 1 a) answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33GBuxILvm4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AtRjDWfnyXT"
      },
      "source": [
        "## Convolutional Layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Z_teqrnyXT"
      },
      "source": [
        "To create a 2D convolutional layer use `keras.layers.Conv2D()` <br>\n",
        "(https://keras.io/api/layers/convolution_layers/convolution2d/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozcTzKjenyXT"
      },
      "source": [
        "**Task 1 b)**    \n",
        "Create a convolutional layer with 32 filters and `kernel_size` `(3,3)`. <br>\n",
        "Apply it to `images[0:1]` and explain the shape of the output. **Do not** <br>\n",
        "**explicitly pass any filters** this time. Instead, use the default random <br>\n",
        "initialization called **glorot_uniform** which pulls numbers from a uniform <br>\n",
        "distribution that goes like the input and output dimensions of each filter.\n",
        "\n",
        "You can plot the resulting images if you want (for example `plot_image` <br>\n",
        "`(new_images[0,:,:,0])`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lqXyHG0nyXT"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a16ic-zCx5ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YhMActnnyXT"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wydjADThnyXU"
      },
      "source": [
        "## Cropping the Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEkeGjS2nyXU"
      },
      "outputs": [],
      "source": [
        "cropped_images = np.array([crop(image) for image in images], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5q8ZciH-nyXU"
      },
      "outputs": [],
      "source": [
        "plot_image(crop(outputs[0, :, :, 0]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcuZMTyUnyXU"
      },
      "source": [
        "## Task 2: Max Pooling Layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBp3x3JwnyXU"
      },
      "source": [
        "Pooling layers are used to **shrink the input image** in order to reduce the <br> computational load, the memory usage, and the number of parameters. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZR0GdtTgnyXU"
      },
      "source": [
        "**Task 2 a)** \n",
        "- Create a max pool layer of pool_size=2 <br>\n",
        "(https://keras.io/api/layers/pooling_layers/max_pooling2d/)\n",
        "- apply the max pool layer to the `cropped_images` assigning the result to the <br>\n",
        "variable `output`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S6C3U3HnyXU"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ud_iKXZ_nyXV"
      },
      "outputs": [],
      "source": [
        "#max_pool_layer = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d7kOlyEnyXV"
      },
      "outputs": [],
      "source": [
        "#output = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76NSKunRnyXV"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gey5ZonyXV"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 8))\n",
        "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.set_title(\"Input\", fontsize=14)\n",
        "ax1.imshow(cropped_images[0])  # plot the 1st image\n",
        "ax1.axis(\"off\")\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.set_title(\"Output\", fontsize=14)\n",
        "ax2.imshow(output[0])  # plot the output for the 1st image\n",
        "ax2.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1e9PI-0nyXV"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 1])\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.set_title(\"Input\", fontsize=14)\n",
        "ax1.imshow(cropped_images[1])\n",
        "ax1.axis(\"off\")\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.set_title(\"Output\", fontsize=14)\n",
        "ax2.imshow(output[1])\n",
        "ax2.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acpPMq18nyXV"
      },
      "source": [
        "**Task 2 b)**\n",
        "\n",
        "Describe the effect of the max pooling layer. What are its benefits for a <br> Neural Network? What are the downsides?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbQ1bl-4nyXW"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjsFK3_6nyXW"
      },
      "source": [
        "Task 2b) answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw6KiAHEnyXW"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3axR-JoGnyXW"
      },
      "source": [
        "# Tackling Fashion MNIST With a CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnC3XAPwnyXW"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "# normalization\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxEwZh2anyXW"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D,\n",
        "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=128, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=10, activation='softmax'),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7tky05bnyXW"
      },
      "source": [
        "### Visualization of Model Structure\n",
        "This is not necessary, but maybe interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So0AwIjtnyXW"
      },
      "outputs": [],
      "source": [
        "pip install visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLAGkHTjnyXX"
      },
      "outputs": [],
      "source": [
        "import visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29aRAZW7nyXX"
      },
      "outputs": [],
      "source": [
        "visualkeras.layered_view(model, legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDXGl_wcnyXX"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, show_shapes=True)   # does not need visual keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolfu3RfnyXX"
      },
      "source": [
        "## Task 3: \n",
        "- Compile the model using `\"sparse_categorical_crossentropy\"` as `loss`, <br> `\"nadam\"` as optimizer and `[\"accuracy\"]` for `metrics` <br>\n",
        "- fit the model for 10 epochs using `[X_valid, y_valid]` as validation data <br>\n",
        "- `evaluate` the model on `X_test, y_test` <br>\n",
        "- predict the first 10 instances of `X_test` and compare them to `y_test` <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKvtUt3bnyXY"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhQ90WZ8nyXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm3KrClqnyXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDBWC05enyXY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1yFXkwXnyXY"
      },
      "source": [
        "## Task 4: ResNet-34\n",
        "\n",
        "ResNet is built on the idea of using **residual connections** between early layers <br>\n",
        "and later layers that the early layers are **not directly attached** to. In effect, <br>\n",
        "this results in **more total connections** in the neural network without actually <br>\n",
        "having to add additional weights to the model. \n",
        "\n",
        "Imagine you had a function that required **100 coefficients with 10 operations**. <br> \n",
        "Instead, you realize that many of the coefficients are **related** to one another <br> \n",
        "so you decide to **recycle terms** and  instead **include more recursive operations**. <br> \n",
        "Now your function has **20 coefficients** but you're doing **30 operations**.\n",
        "\n",
        "\n",
        "This is the idea of ResNet. We **reuse the same weights** multiple times but <br> \n",
        "connecting them to **different layers** each time. This can lead to models that <br>\n",
        "are on the order of 5+ times smaller without meaningfully reducing performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4vxoDeWnyXY"
      },
      "outputs": [],
      "source": [
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"SAME\", use_bias=False)\n",
        "\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                keras.layers.BatchNormalization()]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD6sxPSInyXZ"
      },
      "outputs": [],
      "source": [
        "resnet = keras.models.Sequential()\n",
        "resnet.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
        "                        input_shape=[28,28,1]))\n",
        "resnet.add(keras.layers.BatchNormalization())\n",
        "resnet.add(keras.layers.Activation(\"relu\"))\n",
        "resnet.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    resnet.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "resnet.add(keras.layers.GlobalAvgPool2D())\n",
        "resnet.add(keras.layers.Flatten())\n",
        "resnet.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyN08HDynyXZ"
      },
      "outputs": [],
      "source": [
        "# keras.utils.plot_model(resnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8F-ioI1nyXZ"
      },
      "outputs": [],
      "source": [
        "resnet.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1fibOQGnyXZ"
      },
      "source": [
        "**Task 4:**   <br>\n",
        "a) Compile the ResNet-34 model with ADAM optimizer and train 10 for epochs <br>\n",
        "b) Compare the performance the results with the ones from Task 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEJZkKc1nyXZ"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0DLVEdnyXZ"
      },
      "outputs": [],
      "source": [
        "# resnet.compile(PUT YOUR PARAMETERS HERE)\n",
        "# next, fit the model to the training set\n",
        "# evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNAoHlRHnyXZ"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnZ8DG4lnyXZ"
      },
      "source": [
        "## Task 5: Pretrained Models for Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ8LWxR6nyXa"
      },
      "source": [
        "In this section we use the [tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers) dataset. <br>\n",
        "\n",
        "We reuse a pretrained [Xception model](https://towardsdatascience.com/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568) to classify pictures of flowers. <br> \n",
        "During the first train the weights of the pretrained layers are frozen, while <br> \n",
        "for the second train are not frozen.\n",
        "\n",
        "**WARNING: Do NOT attempt to run the code in this section during the class time.** <br>\n",
        "**These models may take over an hour to train.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6zEGdpTnyXa"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBt7nNu0nyXa"
      },
      "outputs": [],
      "source": [
        "info.splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHUn4c7knyXa"
      },
      "outputs": [],
      "source": [
        "class_names = info.features[\"label\"].names\n",
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnmmC0zAnyXa"
      },
      "outputs": [],
      "source": [
        "n_classes = info.features[\"label\"].num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DxJ8k6-nyXa"
      },
      "outputs": [],
      "source": [
        "dataset_size = info.splits[\"train\"].num_examples\n",
        "dataset_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV1GG-C3nyXa"
      },
      "source": [
        "**Warning:** TFDS's split API has evolved since the book was published. The <br> \n",
        "[new split API](https://www.tensorflow.org/datasets/splits) (called S3) is much <br> \n",
        "simpler to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy1Nd9Z3nyXb"
      },
      "outputs": [],
      "source": [
        "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT05rDR2nyXb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "index = 0\n",
        "for image, label in train_set_raw.take(9):\n",
        "    index += 1\n",
        "    plt.subplot(3, 3, index)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Class: {}\".format(class_names[label]))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StU2xC7KnyXb"
      },
      "outputs": [],
      "source": [
        "def central_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
        "    top_crop = (shape[0] - min_dim) // 4\n",
        "    bottom_crop = shape[0] - top_crop\n",
        "    left_crop = (shape[1] - min_dim) // 4\n",
        "    right_crop = shape[1] - left_crop\n",
        "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
        "\n",
        "def random_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
        "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
        "\n",
        "def preprocess(image, label, randomize=False):\n",
        "    if randomize:\n",
        "        cropped_image = random_crop(image)\n",
        "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "    else:\n",
        "        cropped_image = central_crop(image)\n",
        "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label\n",
        "\n",
        "batch_size = 32\n",
        "train_set = train_set_raw.shuffle(1000).repeat()\n",
        "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
        "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
        "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJYKck3FnyXb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "for X_batch, y_batch in train_set.take(1):\n",
        "    for index in range(9):\n",
        "        plt.subplot(3, 3, index + 1)\n",
        "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
        "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2RMr3NWnyXb"
      },
      "outputs": [],
      "source": [
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "avg    = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model  = keras.models.Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E_uRbo0nyXb"
      },
      "outputs": [],
      "source": [
        "#keras.utils.plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRN1ZP4fnyXb"
      },
      "outputs": [],
      "source": [
        "# base_model not trainable\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, weight_decay=0.01)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_set,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7x7x4SCnyXc"
      },
      "outputs": [],
      "source": [
        "# base_model trainable\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
        "                                 nesterov=True, weight_decay=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_set,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7LliUNAnyXc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "for X_batch, y_batch in test_set.take(1):\n",
        "    for index in range(9):\n",
        "        plt.subplot(3, 3, index + 1)\n",
        "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
        "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bJitAwhnyXc"
      },
      "source": [
        "**Task 5:**\n",
        "- Task 5a) Explain transfer learning and its benefits\n",
        "- Task 5b) Compare the two trainings above (with/without base model trainable). <br> \n",
        "What is the difference and which one performs better here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gf_t0f7nyXc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXvb-PgunyXc"
      },
      "source": [
        "Task 5a) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayfMeBtvnyXc"
      },
      "source": [
        "Task 5b) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwj0IInnnyXc"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEJwz1R8nyXc"
      },
      "source": [
        "# Task 6: High Accuracy CNN for MNIST\n",
        "Build your own CNN and try to achieve the highest possible accuracy on MNIST. <br>\n",
        "A basic structure is given below, play around with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUWJpQPnyXc"
      },
      "source": [
        "The following model uses 2 convolutional layers, followed by 1 pooling layer, <br> \n",
        "then dropout 25%, then a dense layer, another dropout layer but with 50% <br> dropout, and finally the output layer. It reaches about 99.2% accuracy on the <br> \n",
        "test set. This places this model roughly in the top 20% in the <br>\n",
        "[MNIST Kaggle competition](https://www.kaggle.com/c/digit-recognizer/) \\(if we ignore the models with an accuracy greater <br> \n",
        "than 99.79% which were most likely trained on the test set, as explained by <br> \n",
        "Chris Deotte in [this post](https://www.kaggle.com/c/digit-recognizer/discussion/61480)). \n",
        "\n",
        "In order to reach an accuracy higher than 99.5% on the test set you might try:\n",
        "\n",
        "a) batch normalization layers<br> \n",
        "(https://keras.io/api/layers/normalization_layers/batch_normalization/)   \n",
        "b) set a learning scheduler (Check Chapter 11)<br>\n",
        "c) add image augmentation (Check Chapter 14)<br>\n",
        "d) create an ensemble (Check Chapter 14)<br>\n",
        "e) use hyperparameter tuning, e.g. with [keras tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) (Hyperband seems to <br>\n",
        "work quite well) \n",
        "\n",
        "As long as you implement at least **two** of the above you will get full points <br> \n",
        "on this one. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFfeMKtKnyXd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}