{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp23/blob/main/Exercises/03_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfmAJSqwE0ys"
      },
      "source": [
        "# Hands On Exercise 3: Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLbEKL7BLvmh"
      },
      "source": [
        "**Chapter 6 – Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN3OdD0LLvmk"
      },
      "source": [
        "Due date: 2023-02-14\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and <br> \n",
        "Linus Torvalds it would be <br>\n",
        "\"03_Exercise3_Goup42_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown <br> and possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not <br>\n",
        "intended for you to try to understand every line of code.   <br>\n",
        "Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Py2NOeBLvml"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKxv51qSLvmm"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slm-PTBnLvmn"
      },
      "source": [
        "# Loading the data\n",
        "\n",
        "You might remember the [Iris Flower Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set) from last lecture. If <br>\n",
        "you're curious about the data set, feel free to check out the wikipedia <br>\n",
        "article in the hotlink listed here. Basically, it contains information <br>\n",
        "about several species of iris flowers along with a classification of <br>\n",
        "which species of iris they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHKUSfoPLvmn"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] # petal length and width\n",
        "y = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAG_6mzJLvmo"
      },
      "outputs": [],
      "source": [
        "X.shape  # 150 instances, 2 features per instance (petal length and width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esMJWWLaLvmp"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqN-n6Z_Lvmq"
      },
      "source": [
        "Now that we have the data we can fit a model to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR5pE8RNLvmq"
      },
      "source": [
        "# Decision Trees for Classification\n",
        "\n",
        "At their core, **decision trees are branching networks** that take data and <br>\n",
        "pose a series of rules-based questions at each branching point in the <br>\n",
        "tree. **A branch might look like the following: if the number is greater** <br>\n",
        "**two, take the left branch. Otherwise, take the right branch**. The idea is <br>\n",
        "to **construct a network of these rules-based conditions that**, when <br>\n",
        "imposed on your input data, **get you to a desired output**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huyiT7KXLvmq"
      },
      "source": [
        "### Task 1\n",
        "\n",
        "Build a simple [decision tree classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) with `max_depth` of 2 and <br> \n",
        "`random_state` of 42. Fit to the data `X`, `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtBJvshWLvmr"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzACHvMk2hd6"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0su6wPoLLvmr"
      },
      "outputs": [],
      "source": [
        "# tree_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sam0EOraLvmr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHZcxPr2tqY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAPW_1OxLvms"
      },
      "source": [
        "### Visualization of the Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrWXPwvvLvms"
      },
      "source": [
        "**Note** Next step requires the GraphViz executable to be installed. Since <br> \n",
        "this is a lot of additional work, it is recommended to work through a <br> \n",
        "Colab notebook instead. However, if you'd like to work locally, <br> follow the steps below. \n",
        "\n",
        "**YOU ONLY NEED TO DO THE FOLLOWING IF YOU'RE WORKING OUTSIDE OF COLAB**\n",
        "\n",
        "To install GraphViz, download it here: https://graphviz.org/download/\n",
        "\n",
        "During the installation, make sure its path was added to the Path <br>\n",
        "environmental variable. For your user, add the path to the bin folder, <br> \n",
        "for the system environmental variable, add the path to the  ```dot.exe``` <br>\n",
        "file. \n",
        "```\n",
        "C:\\Program Files (x86)\\Graphviz2.38\\bin\n",
        "C:\\Program Files (x86)\\Graphviz2.38\\bin\\dot.exe\n",
        "``` \n",
        "\n",
        "Then, you can use pip or conda to install the package: ```pip install``` <br> \n",
        "```graphiz``` or ```conda install graphiz```\n",
        "\n",
        "Finally, close your Jupyter notebook, close the command promt executing <br> \n",
        "it and restart both. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gziiuAgLvmt"
      },
      "outputs": [],
      "source": [
        "from graphviz import Source\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "IMAGES_PATH = os.path.join(\".\", \"images\")\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "export_graphviz(\n",
        "        tree_clf,\n",
        "        out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
        "        feature_names=iris.feature_names[2:],\n",
        "        class_names=iris.target_names,\n",
        "        rounded=True,\n",
        "        filled=True\n",
        "    )\n",
        "\n",
        "Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't worry about understanding the following code but do try to** <br>\n",
        "**understand the plot itself**. This isn't the first time we've looked at <br>\n",
        "plots of decision boundaries before. Reminder, a decision boundary is <br>\n",
        "some cutoff point we determine within the data that separates one <br>\n",
        "classification from another."
      ],
      "metadata": {
        "id": "W1AhD8IqILWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62w-roFsLvmt"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if not iris:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    if plot_training:\n",
        "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
        "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\n",
        "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris virginica\")\n",
        "        plt.axis(axes)\n",
        "    if iris:\n",
        "        plt.xlabel(\"Petal length\", fontsize=14)\n",
        "        plt.ylabel(\"Petal width\", fontsize=14)\n",
        "    else:\n",
        "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
        "    if legend:\n",
        "        plt.legend(loc=\"lower right\", fontsize=14)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
        "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
        "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
        "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
        "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
        "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
        "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG5_hSo9Lvmu"
      },
      "source": [
        "## Predicting classes and class probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoFdkAfPLvmu"
      },
      "source": [
        "### Task 2\n",
        "Next, predict the probabilities and the class for the following values: <br> \n",
        "``` X=[[5, 1.5]] ``` for petal length and petal width.\n",
        "\n",
        "You will need the functions `tree_clf.predict_proba()` and <br>\n",
        "`tree_clf.predict()`. You do not need to import these as these are <br>\n",
        "functions already included with your decision tree classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjoh3xeA36Rc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGOKuN7zLvmu"
      },
      "outputs": [],
      "source": [
        "# pred_prob = \n",
        "# pred_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI9sJ915Lvmu"
      },
      "outputs": [],
      "source": [
        "# y_pred = \n",
        "# y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UKK78g38bM"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diwf_pKgLvmv"
      },
      "source": [
        "You can also check in the plot above what class you would put the instance <br> `X=[[5, 1.5]]` into.\n",
        "In the plot, yellow corresponds to class 0 (Iris setosa), blue <br> corresponds to class 1 (Iris versicolor), and green corresponds to class <br> 2 (Iris virginica)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06craaUpLvmv"
      },
      "source": [
        "## Sensitivity to training set details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T9FESIKLvmv"
      },
      "source": [
        "### Task 3\n",
        "Next, we want to explore the sensitivity to the training set details. <br> \n",
        "We will train the same decision tree model, but with slightly different <br> \n",
        "training data.\n",
        "\n",
        "\n",
        "**Task 3a (bonus)**: The statements in the next two cells are the same. <br>\n",
        "For bonus points you can explain why that's the case. \n",
        "\n",
        "Hint: refer to the [dataset description](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)\n",
        "and think about the slicing of the <br> \n",
        "array below. Which values are selected and which data do they show? It <br> \n",
        "helps to evaluate parts of the expression separately and try to <br>\n",
        "understand them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b6kbQlWLvmv"
      },
      "outputs": [],
      "source": [
        "X[(X[:, 1]==X[:, 1][y==1].max()) & (y==1)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[(X[:, 1]==1.8]) & (y==1)]"
      ],
      "metadata": {
        "id": "l2etXft1MQ9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXfv9e_SLvmv"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y9x_e-tLvmv"
      },
      "source": [
        "Task 3a (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ8fLtTKLvmw"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EatTXtOLvmw"
      },
      "source": [
        "**Task 3b**: Now, what if we select the values that are NOT <br>\n",
        "1.8. Which data would the dataset represent? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24q8LbZLvmw"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA4_p-1NLvmw"
      },
      "source": [
        "Task 3b answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_we9tatLvmw"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9Ih_SPJLvmw"
      },
      "outputs": [],
      "source": [
        "not_1_8 = (X[:, 1]!=1.8) | (y==2)\n",
        "X_tweaked = X[not_1_8]\n",
        "y_tweaked = y[not_1_8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2R8FoyfLvmx"
      },
      "source": [
        "**Task 3c**: Fit a new Decision Tree Classifier to these values <br> (`X_tweaked`, `y_tweaked`) with the same initial parameter values <br>\n",
        "```max_depth = 2``` and ```random_state = 42```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmkWDe7wLvmx"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vohlqSe7Lvmx"
      },
      "outputs": [],
      "source": [
        "# tree_clf_tweaked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edgT5nsVLvmx"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaL72ABNLvmx"
      },
      "source": [
        "So now we have trained a new decision tree `tree_clf_tweaked` that has <br> \n",
        "slightly different training data **(actually only *one* element less)**. <br>\n",
        "Let's visualize the new decision tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SXLFWekLvmx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend=False)\n",
        "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)\n",
        "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
        "plt.text(1.0, 0.9, \"Depth=0\", fontsize=15)\n",
        "plt.text(1.0, 1.80, \"Depth=1\", fontsize=13)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KDj1inqLvmy"
      },
      "source": [
        "**Task 3d**: Describe how the new decision tree is different from the <br> \n",
        "one before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kscTnPS1Lvmy"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WyF_v-9Lvmy"
      },
      "source": [
        "Task 3d answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBN0Ja4NLvmy"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_taL5yIyLvmy"
      },
      "source": [
        "# Regression Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWCFARlILvmy"
      },
      "source": [
        "In this subsection, we will be working with a regression task. <br> \n",
        "**Notice that the dataset below is different**. Here we are using a  <br>\n",
        "**training set generated by a quadratic function with the addition of** <br>\n",
        "**some random noise**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_680ZX4Lvmy"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 200\n",
        "X = np.random.rand(m, 1)\n",
        "y = 4 * (X - 0.5) ** 2\n",
        "y = y + np.random.randn(m, 1) / 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT4eSc39Lvmy"
      },
      "source": [
        "### Task 4\n",
        "Similar to what you've done above, create a [decision tree regressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) <br>\n",
        "with the same hyperparameters as the classifier. \n",
        "  - `max_depth=2`\n",
        "  - `random_state=42`\n",
        "\n",
        "Fit it to `X`, `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0P4Z_ivLvmy"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5IeVa0P6k0E"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGp1S4tkLvmz"
      },
      "outputs": [],
      "source": [
        "# tree_reg = "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mp3jTYr12JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9Bq2js73A4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqv5xTc7Lvmz"
      },
      "source": [
        "Below, you'll see the difference in the prediction for an increased <br> \n",
        "value of the ```max_depth``` parameter. What can you say about the <br> results?\n",
        "\n",
        "**Again, the important thing is to understand the plots but not** <br>\n",
        "**necessarilly all of the code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6-d4F5PLvmz"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
        "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
        "tree_reg1.fit(X, y)\n",
        "tree_reg2.fit(X, y)\n",
        "\n",
        "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
        "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
        "    y_pred = tree_reg.predict(x1)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    if ylabel:\n",
        "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_regression_predictions(tree_reg1, X, y)\n",
        "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
        "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
        "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
        "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
        "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
        "plt.legend(loc=\"upper center\", fontsize=18)\n",
        "plt.title(\"max_depth=2\", fontsize=14)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
        "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
        "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
        "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
        "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
        "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
        "plt.title(\"max_depth=3\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3crrk6E0Lvm0"
      },
      "outputs": [],
      "source": [
        "export_graphviz(\n",
        "        tree_reg1,\n",
        "        out_file=os.path.join(IMAGES_PATH, \"regression_tree.dot\"),\n",
        "        feature_names=[\"x1\"],\n",
        "        rounded=True,\n",
        "        filled=True\n",
        "    )\n",
        "Source.from_file(os.path.join(IMAGES_PATH, \"regression_tree.dot\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxasJyRFLvm0"
      },
      "source": [
        "## Restricting the Tree with `min_samples_leaf`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4jVUrTPLvm0"
      },
      "source": [
        "Here we are restricting the tree to `min_samples_leaf=10`. You can read <br> about it in [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) but basically it is adding the requirement <br>\n",
        "that, **before we make a new leaf branch, we need 10 data points that fit** <br>\n",
        "**the rule that would lead to that branch**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u__spHoSLvm0"
      },
      "outputs": [],
      "source": [
        "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
        "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
        "tree_reg1.fit(X, y)\n",
        "tree_reg2.fit(X, y)\n",
        "\n",
        "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
        "y_pred1 = tree_reg1.predict(x1)\n",
        "y_pred2 = tree_reg2.predict(x1)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "\n",
        "plt.sca(axes[0])\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "plt.axis([0, 1, -0.2, 1.1])\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
        "plt.legend(loc=\"upper center\", fontsize=18)\n",
        "plt.title(\"No restrictions\", fontsize=14)\n",
        "\n",
        "plt.sca(axes[1])\n",
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "plt.axis([0, 1, -0.2, 1.1])\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfioJ6bhLvm0"
      },
      "source": [
        "### Task 5\n",
        "\n",
        "Discuss the effect of the restrictions on the regression results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pu8O_PnLvm1"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwwXiX4cLvm1"
      },
      "source": [
        "Task 5 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBE7oVb9Lvm1"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3IfDqQRLvm1"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WnVP54rLvm1"
      },
      "source": [
        "In this exercise you will be using the [make_moons](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) dataset. You can <br>\n",
        "see a plot below. Basically, the tool generates fake data of two inter- <br>\n",
        "leaving half-circles or \"moons\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDumL4RnLvm1"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQeS4I_nLvm1"
      },
      "outputs": [],
      "source": [
        "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap=cm_bright, edgecolors=\"k\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use train_test_split to split our fake data into 20% test and <br>\n",
        "80% train data."
      ],
      "metadata": {
        "id": "1ElUuEJIsylC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn5Cxp6lLvm2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLtSoaJXLvm2"
      },
      "outputs": [],
      "source": [
        "tree_moons = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
        "                       max_features=None, max_leaf_nodes=17,\n",
        "                       min_impurity_decrease=0.0, \n",
        "                       min_samples_leaf=1, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, \n",
        "                       random_state=42, splitter='best')\n",
        "tree_moons.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uztDNVn4Lvm2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred_moons = tree_moons.predict(X_test)\n",
        "accuracy_score(y_test, y_pred_moons)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28IASpWkLvm2"
      },
      "source": [
        "The decision tree trained with all data has an accuracy as shown above. <br>\n",
        "Now we will build a forest consisting of trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF7rD4OdLvm2"
      },
      "source": [
        "We will be building 100 `DecisionTreeClassifier`s. <br>\n",
        "First we do a train-test-split to get training and test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdooBsL9Lvm2"
      },
      "source": [
        "As you can see, the training data has a size of 8000 instances. <br>\n",
        "Let's generate 1000 subsets (mini-sets) of `X_train`, each containing <br> \n",
        "100 instances selected randomly. You may have noticed that that's more\n",
        "<br>\n",
        "than 8000. We'll be reusing multiple datapoints.\n",
        "\n",
        "Then we will train a separate tree on each of the mini-sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CHBb3ZHLvm2"
      },
      "source": [
        "## Task 6\n",
        "\n",
        "Grow a forest.<br>\n",
        "You will have to get the following done, the way you implement it is <br> \n",
        "your choice:\n",
        "\n",
        "- Split `X_train` into 1000 subsets, each containing 100 instances <br> selected randomly. You can use sklearn's [ShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html) for this. \n",
        "- Train one [Decision Tree](https://scikit-learn.org/0.17/modules/generated/sklearn.tree.DecisionTreeClassifier.html) on each subset. The hyperparameter values <br> \n",
        "below work well:\n",
        "```python\n",
        "class_weight=None, criterion='gini', max_depth=None,\n",
        "                       max_features=None, max_leaf_nodes=17,\n",
        "                       min_samples_leaf=1, min_samples_split=2,\n",
        "                       min_weight_fraction_leaf=0.0, \n",
        "                       random_state=42, splitter='best'\n",
        "```\n",
        "You might need `from sklearn.base import clone`. To clone the tree 1000 <br> times.\n",
        "- Calculate the accuracy on the test data `X_test`, `y_test` for each <br> tree. What is the mean accuracy?\n",
        "- Build a forest: For each test set instance, generate the predictions <br> of the 1,000 Decision Trees, and keep only the most frequent <br> prediction (you can use SciPy's `scipy.stats.mode()` function for <br>\n",
        "this). This gives you _majority-vote predictions_ over the test set. <br> \n",
        "What is the accuracy of your forest? You should get a slightly better <br> accuracy than the one tree trained on all training data.\n",
        "\n",
        "\n",
        "If you struggle with this task for too long, then you can find the solution [here](https://github.com/ageron/handson-ml2/blob/master/06_decision_trees.ipynb) <br> \n",
        "under 8 (bottom of the notebook).\n",
        "\n",
        "**In order you should**:\n",
        "- Attempt to solve this as a group by looking up code documentation\n",
        "- Raise your hand and ask for help\n",
        "- Check the official book solutions only for parts of the task you are <br> struggling with\n",
        "- Look at, understand, and try to replicate the code from the solution\n",
        "\n",
        "**Do NOT directly copy the entire solution into this notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HMNRC-7Lvm3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.base import clone\n",
        "from scipy.stats import mode\n",
        "\n",
        "\n",
        "n_trees = 1000\n",
        "n_instances = 100\n",
        "mini_sets = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsnymzLuLvm2"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L33GBuxILvm4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nav_menu": {
      "height": "309px",
      "width": "468px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}