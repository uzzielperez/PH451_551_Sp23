{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp23/blob/main/Exercises/02_Training_linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWEunswqrzWp"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and <br> \n",
        "Linus Torvalds it would be <br>\n",
        "\"02_Exercise2_Goup42_Stallman_Torvalds.pdf\".<br>\n",
        "\n",
        "Submission via blackboard (UA).<br>\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown <br>\n",
        "and possibly $\\LaTeX{}$ if you want to."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRSpWkx0rzWx"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrId8lV7rzWx"
      },
      "source": [
        "First, let's import a few common modules, ensure MatplotLib plots <br> figures inline and prepare a function to save the figures. We also <br> check that Python 3.5 or later is installed (although Python 2.x may <br> \n",
        "work, it is deprecated so we strongly recommend you use Python 3 <br> instead), as well as Scikit-Learn ≥0.20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZytlnYYrzWy"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c5b04e9rzW0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGEkqfDPrzW2"
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3XwzO8KBnX9"
      },
      "source": [
        "# Tasks 1-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PRAKeQ2rzW3"
      },
      "source": [
        "### Task 1\n",
        "Fit a linear [regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) `lin_reg` and print out its intercept and coefficient. <br>\n",
        "\n",
        "To start with, create a LinearRegression model object then use <br>\n",
        "`lin_reg.fit(X, y)` to fit the model to the data sets X and y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEupw4u4rzW4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzACHvMk2hd6"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZEguB-ArzW5"
      },
      "outputs": [],
      "source": [
        "# lin_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHZcxPr2tqY"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FptcwTMhrzW5"
      },
      "source": [
        "### Task 2 \n",
        "With the model you created above, [predict](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) the new values `X_new`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEYoEbYRrzW6"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([[0], [2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjoh3xeA36Rc"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22Frl1mtrzW7"
      },
      "outputs": [],
      "source": [
        "# y_predict = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UKK78g38bM"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5v1vgjirzW7"
      },
      "source": [
        "Let's plot the data (blue) and our predictions (red) using matplotlib.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV7ZcG7PrzW8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.plot(X, y, \"b.\")\n",
        "plt.plot(X_new, y_predict, \"r-\", markersize = 20)\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq0SLqyyrzW9"
      },
      "source": [
        "### Task 3\n",
        "The `LinearRegression` class is based on the `scipy.linalg.lstsq()` <br> \n",
        "function (the name stands for \"least squares\"). In essence, it attempts <br>\n",
        "to fit a line by minimize the sum of squares of residuals (true - pred) <br>\n",
        "for all points in a data set.\n",
        "\n",
        "Basically we fit to the form $y = 1 \\Theta_0 + \\Theta_1 x = x_b\\cdot \\Theta$ with $x_b = (1, x)^T$ and <br> \n",
        "$\\Theta = (\\Theta_0, \\Theta_1)^T$. So we need to add a 1 to each instance of `X`. See slide 13 of <br>\n",
        "Lecture 4.\n",
        "\n",
        "Look up [`np.linalg.lstsq()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html), call it directly on `X_b`, `y` and print out the $\\Theta$ you <br> \n",
        "found this way. Compare the output with the output from your <br> `LinearRegression` model above. Set `rcond=1e-6` in `np.linalg.lstsq`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pulqnD5VrzW9"
      },
      "outputs": [],
      "source": [
        "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
        "X_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5IeVa0P6k0E"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jphg9m57kvE"
      },
      "outputs": [],
      "source": [
        "# theta_best_svd, residuals, rank, s = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o9Bq2js73A4"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG9puAEZ6anz"
      },
      "outputs": [],
      "source": [
        "# these are the values from the linear regression model\n",
        "# compare them to what you got above\n",
        "lin_reg.intercept_, lin_reg.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQfG16MYrzW-"
      },
      "source": [
        "# Linear regression using batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CLJrRFErzW-"
      },
      "source": [
        "Just click through this section.\n",
        "\n",
        "The first part shows how to manually do gradient descent. The formula <br> \n",
        "for the gradients has been calculated by hand and we just plug in `X`<br> \n",
        "and `theta`.\n",
        "\n",
        "The second part (function `plot_gradient_descent`) does the gradient <br> \n",
        "descent and plots the first 10 steps.\n",
        "\n",
        "[Here](https://github.com/Jaewan-Yun/optimizer-visualization) is a nice visualization of different gradient descent methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUZNzC6grzW_"
      },
      "outputs": [],
      "source": [
        "eta = 0.1  # learning rate\n",
        "n_iterations = 1000 # number of iterations\n",
        "m = 100 # the number of items in X_b\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "print(theta) # printing what our initial theta values look like\n",
        "\n",
        "# The following loop is the entire gradient descent algorithm for this\n",
        "# linear regression example. As a bonus question below you can attempt to \n",
        "# describe what is actually happening here.\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
        "    theta = theta - eta * gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the change in our value for theta after running this algorithm."
      ],
      "metadata": {
        "id": "MXp-XEM-JLSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw5Wt52LrzW_"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't worry too much about what all of the code below is doing. In <br>\n",
        "short, it's **running the gradient descent algorithm and updating a plot** <br>\n",
        "with increasingly better-fitted regression lines."
      ],
      "metadata": {
        "id": "8O1NbQtXJRJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyHa0cgrrzXA"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = []\n",
        "\n",
        "def plot_gradient_descent(theta, eta, theta_path=None):\n",
        "    m = len(X_b)\n",
        "    plt.plot(X, y, \"b.\")\n",
        "    n_iterations = 1000\n",
        "    X_new = np.array([[0], [2]])\n",
        "    X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "    for iteration in range(n_iterations):\n",
        "        if iteration < 10:\n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if iteration > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "        theta = theta - eta * gradients   # and this one\n",
        "        if theta_path is not None:\n",
        "            theta_path.append(theta)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 2, -10, 15])\n",
        "    plt.title(r\"$\\eta = {}$\".format(eta), fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've made a function to do gradient descent and plot our <br>\n",
        "regressions, we can **test out a few different learning rate** <br>\n",
        "**hyper-parameters**."
      ],
      "metadata": {
        "id": "rS1iC02wKqYy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9UP42oZrzXA"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42) # this keeps your random outputs consistent each time\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "theta_path_bgd = []\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(131); plot_gradient_descent(theta, eta=0.02)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(132); plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)\n",
        "plt.subplot(133); plot_gradient_descent(theta, eta=0.5)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXB606II9NZK"
      },
      "source": [
        "For a learning rate $\\eta$ that is too small we approach the minimum too slowly and for a too large learning rate we jump around the minimum like the purple curve in [this animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif) from [this repo](https://github.com/Jaewan-Yun/optimizer-visualization) on GitHub.\n",
        "Note that this animation does not correspond to our data.\n",
        "![animation](https://github.com/Jaewan-Yun/optimizer-visualization/blob/master/figures/movie9.gif?raw=true) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pPLGUcLDB_w"
      },
      "source": [
        "### Task 3.5 (Bonus)\n",
        "Explain what happens in the two lines of code\n",
        "```python\n",
        "gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)   # this line\n",
        "theta = theta - eta * gradients   # and this one\n",
        "```\n",
        "in the code above. Also explain where the first line comes from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLQeFXCqDkhK"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4SGu63sDuHO"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 3.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a-Q3Y0VDr1P"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1ZmZKvrzXA"
      },
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amR_zg9OrzXB"
      },
      "source": [
        "This section shows how one can implement Stochastic Gradient Descent by <br> \n",
        "hand. You might notice that **this looks a lot like the gradient descent** <br>\n",
        "**method** from above. The difference here, is that **we're adding stochacity** <br>\n",
        "**or randomness**. We do this by **performing gradient descent on a random** <br>\n",
        "**subset** of all data points for each step **rather than on the entire data** <br>\n",
        "**set** every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-8RC0w2rzXB"
      },
      "outputs": [],
      "source": [
        "theta_path_sgd = [] # a list where we can store parameters of our fit\n",
        "m = len(X_b) # the number of items in our data set\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the lines below that are marked **#here**. Those lines are where **we** <br>\n",
        "**introduce stochasticity by picking out a random single data point**. <br>\n",
        "**Don't worry too much about whether you understand what's happening in** <br>\n",
        "**this code**. In practice, we'll usually use other tools to implement this <br>\n",
        "automatically rather than code it by hand."
      ],
      "metadata": {
        "id": "k-exggOfRQoA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpdXF8RGrzXB"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50 # number of times to fully iterate over our data set\n",
        "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
        "\n",
        "# Sometimes we want to be able to change our learning rate over time.\n",
        "# If we start with a learning rate of 1. Our next value would be 5/(1+50).\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "X_new = np.array([[0], [2]])\n",
        "X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(m):\n",
        "        if epoch == 0 and i < 20: \n",
        "            y_predict = X_new_b.dot(theta)\n",
        "            style = \"b-\" if i > 0 else \"r--\"\n",
        "            plt.plot(X_new, y_predict, style)\n",
        "        random_index = np.random.randint(m)   #here\n",
        "        xi = X_b[random_index:random_index+1] #here\n",
        "        yi = y[random_index:random_index+1]   #here\n",
        "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(epoch * m + i) # reduce our learning rate\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_sgd.append(theta) \n",
        "\n",
        "plt.plot(X, y, \"b.\") \n",
        "plt.xlabel(\"$x_1$\", fontsize=18) \n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.axis([0, 2, 0, 15]) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB09p83qrzXB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW3PHeIyrzXC"
      },
      "source": [
        "### Task 4\n",
        "- Build an [SGD Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html) and assign it to `sgd_reg`.\n",
        "- Fit the SGD Regressor `sgd_reg` to `X` and `y` and print out its <br> intercept `sgd_reg.intercept_`and coefficients `sgd_reg.coef_`. \n",
        "- Compare the values you find to the ones from the stochastic gradient <br>\n",
        "descent above.\n",
        "\n",
        "Use `max_iter=50, tol=-np.infty, penalty=None, eta0=0.1, random_state=42` as parameters for the SGD Regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-ZAT9jlrzXC"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYd3w4FUkC"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓\n",
        "your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7uzxyparzXC"
      },
      "outputs": [],
      "source": [
        "# sgd_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgEgY7NUFfwu"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGYI3I2IrzXC"
      },
      "source": [
        "# Mini-batch gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1C9EIZxrzXD"
      },
      "source": [
        "The following code shows how one could implement mini-batch gradient <br> \n",
        "descent by hand. **Remember, the stochastic part of SGD means we picked** <br>\n",
        "**out a random subset of our data**. Above, that was only a single point. <br>\n",
        "How do you think mini-batch gradient descent differs from stochastic <br>\n",
        "gradient descent? As a hint, look for `#pay close attention to this line`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDyGZv2TrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_mgd = []\n",
        "\n",
        "n_iterations = 50\n",
        "minibatch_size = 20\n",
        "\n",
        "np.random.seed(42)\n",
        "theta = np.random.randn(2,1)  # random initialization\n",
        "\n",
        "t0, t1 = 200, 1000\n",
        "def learning_schedule(t):\n",
        "    return t0 / (t + t1)\n",
        "\n",
        "t = 0\n",
        "for epoch in range(n_iterations):\n",
        "    shuffled_indices = np.random.permutation(m)\n",
        "    X_b_shuffled = X_b[shuffled_indices]\n",
        "    y_shuffled = y[shuffled_indices]\n",
        "    for i in range(0, m, minibatch_size):\n",
        "        t += 1\n",
        "        xi = X_b_shuffled[i:i+minibatch_size] #pay close attention to this line\n",
        "        yi = y_shuffled[i:i+minibatch_size]   #pay close attention to this line\n",
        "        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)\n",
        "        eta = learning_schedule(t)\n",
        "        theta = theta - eta * gradients\n",
        "        theta_path_mgd.append(theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN-WG-virzXD"
      },
      "outputs": [],
      "source": [
        "theta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY6CkmGbrzXD"
      },
      "outputs": [],
      "source": [
        "theta_path_bgd = np.array(theta_path_bgd)\n",
        "theta_path_sgd = np.array(theta_path_sgd)\n",
        "theta_path_mgd = np.array(theta_path_mgd)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how these three different approaches to gradient descent <br>\n",
        "compare with one another."
      ],
      "metadata": {
        "id": "9zOmS8USUMyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAlRIsEdrzXE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(7,4))\n",
        "plt.plot(theta_path_sgd[:, 0], theta_path_sgd[:, 1], \"r-s\", linewidth=1, label=\"Stochastic\")\n",
        "plt.plot(theta_path_mgd[:, 0], theta_path_mgd[:, 1], \"g-+\", linewidth=2, label=\"Mini-batch\")\n",
        "plt.plot(theta_path_bgd[:, 0], theta_path_bgd[:, 1], \"b-o\", linewidth=3, label=\"Batch\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "plt.xlabel(r\"$\\theta_0$\", fontsize=20)\n",
        "plt.ylabel(r\"$\\theta_1$   \", fontsize=20, rotation=0)\n",
        "plt.axis([2.5, 4.5, 2.3, 3.9])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vvPe8JMrzXE"
      },
      "source": [
        "### Task 5\n",
        "Explain which Linear Regression training algorithm you can use if you <br> \n",
        "have a training set with millions of features? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HJaiUFt7Usg"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWjF5XmDEtQW"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 5 answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjB4sX9KFlE5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcV_jYLLrzXF"
      },
      "source": [
        "# Polynomial regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn3PrphGrzXE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.random as rnd\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD47wIkqrzXF"
      },
      "source": [
        "With a little trick Linear Regression turns out to be rather powerful <br> also for data that is not linear! <br>\n",
        "The trick is to **add \"new features\" to `X`**. In our case we need a second <br> \n",
        "feature that is just $x^2$. **Then we fit again**, but because **our feature**<br> \n",
        "**vector is now $(1, x, x^2)$**, the **dot product with** $(\\Theta_0, \\Theta_1, \\Theta_2)$ **gives** <br> \n",
        "**the form for the parabola** (check this if you don't see it!).\n",
        "\n",
        "Here we show an example of how to fit a parabola with Linear Regression.\n",
        "\n",
        "**You do not need to understand the code** in detail but we've added <br>\n",
        "comments if you want to understand it better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1uyl-1KrzXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
        "\n",
        "# From the documentation for PolynomialFeatures:\n",
        "# Generate a new feature matrix consisting of all polynomial combinations of \n",
        "# the features with degree less than or equal to the specified degree. For \n",
        "# example, if an input sample is two dimensional and of the form [a, b], the \n",
        "# degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
        "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# We can then fit to and transform our 100 X values to look like a polynomial.\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# LinearRegression fits the polynomials with least squares fitting.\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y)\n",
        "\n",
        "# We can also transform other sets of X values without needing to fit first\n",
        "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
        "X_new_poly = poly_features.transform(X_new)\n",
        "\n",
        "# With some new polynomial X values that the model hasn't seen, we can get\n",
        "# some predicted values.\n",
        "y_new = lin_reg.predict(X_new_poly)\n",
        "\n",
        "# The following for loop is going to repeat for some polynomials of different\n",
        "# degrees and create a regression  plot with corresponding design elements.\n",
        "for style, width, degree in ((\"g-\", 1, 300), (\"b--\", 2, 2), (\"r-+\", 2, 1)):\n",
        "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    std_scaler = StandardScaler()\n",
        "    lin_reg = LinearRegression()\n",
        "    # Pipeline is a tool that lets you apply multiple sklearn layers to data\n",
        "    # in a sequential way. Here, we're transforming our data to be polynomial,\n",
        "    # then scaling the data using standard normalization, and finally running\n",
        "    # a linear regression on the data.\n",
        "    polynomial_regression = Pipeline([\n",
        "            (\"poly_features\", polybig_features),\n",
        "            (\"std_scaler\", std_scaler),\n",
        "            (\"lin_reg\", lin_reg),\n",
        "        ])\n",
        "    # with our Pipeline constructed, we can fit our model to some data.\n",
        "    polynomial_regression.fit(X, y)\n",
        "    y_newbig = polynomial_regression.predict(X_new)\n",
        "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
        "\n",
        "plt.plot(X, y, \"b.\", linewidth=3)\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.title(\"Fitting different polynomials\")\n",
        "plt.axis([-3, 3, 0, 10])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RX8MFNRH8l5"
      },
      "source": [
        "### Task 6\n",
        "- Which curve (red, blue, green) fits the data (blue dots) the best?\n",
        "- Explain what happens to the green curve. \n",
        "<br>Hint: (\"g-\", 1, 300) = style, width, degree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icSe9mOO7W74"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EMq8MKCE7_2"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 6 answer: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e21Gcly27RC5"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KOQXfIBrzXF"
      },
      "source": [
        "# Training and Validation Error\n",
        "\n",
        "**Learning curves are a visualization of your error over time**. Here we're <br>\n",
        "looking at plots of mean-squared error (true-pred)^2 for both our <br>\n",
        "training data and our validation data over the course of training. By <br>\n",
        "**checking BOTH training and validation curves**, we can verify that our <br>\n",
        "model is **learning AND generalizing**.\n",
        "\n",
        "Not all machine learning toolkits keep track of your loss history <br>\n",
        "automatically. Here's an example doing it by hand with lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfE3AUIlrzXF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_learning_curves(model, X, y):\n",
        "    # train_test_split splits data into train and test sets automatically\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    train_errors, val_errors = [], []\n",
        "    for m in range(1, len(X_train)):\n",
        "        model.fit(X_train[:m], y_train[:m])\n",
        "        y_train_predict = model.predict(X_train[:m])\n",
        "        y_val_predict = model.predict(X_val)\n",
        "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
        "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
        "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
        "    plt.legend(loc=\"upper right\", fontsize=14)   \n",
        "    plt.xlabel(\"Training set size\", fontsize=14) \n",
        "    plt.ylabel(\"RMSE\", fontsize=14)              "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quH_toCnrzXG"
      },
      "source": [
        "Let's compare different degrees of polynomials for fitting. We're going <br>\n",
        "to **generate some learning curves for increasingly complicated** <br>\n",
        "**polynomial forms. Degree = 1, 2, 30**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngKw-o-ArzXG"
      },
      "outputs": [],
      "source": [
        "# Linear Regression (degree 1)\n",
        "lin_reg = LinearRegression()\n",
        "plot_learning_curves(lin_reg, X, y)\n",
        "plt.axis([0, 80, 0, 3])                        \n",
        "plt.show()                                     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARFBQLB1rzXG"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Degree 2\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=2, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 2])           \n",
        "plt.show()                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC9gEVpArzXG"
      },
      "outputs": [],
      "source": [
        "# Degree 20\n",
        "polynomial_regression = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=30, include_bias=True)),\n",
        "        (\"lin_reg\", LinearRegression()),\n",
        "    ])\n",
        "\n",
        "plot_learning_curves(polynomial_regression, X, y)\n",
        "plt.axis([0, 80, 0, 3])           \n",
        "plt.show()                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srSUIubHrzXH"
      },
      "source": [
        "### Task 7\n",
        "Look at the three plots above that show the training and validation <br> \n",
        "errors for linear regression, quadratic regression and order 20. <br> Compare them to the plot above called \"Fitting different polynomials\". <br>\n",
        "\n",
        "Describe which one is likely overfitting and which one is likely <br> underfitting and why. \n",
        "\n",
        "Hint: Also take the values of the errors into consideration when <br> comparing the different polynomials and not just the shape of the train <br> \n",
        "and validation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GjY75AyAWL_"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKPlcI1lI4D5"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 7 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpOfr3BKAcTy"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcHILVuOrzXH"
      },
      "source": [
        "# Regularized models\n",
        "\n",
        "**Regularization** is one approach to help prevent overfitting. The general <br>\n",
        "idea is to **apply a penalty to model coefficients which discourages** <br>\n",
        "**overly-complex models**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3c-JnVESrzXH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 20\n",
        "X = 3 * np.random.rand(m, 1)\n",
        "y = 1 + X*X + 1.5 * np.random.randn(m, 1) / 2\n",
        "X_new = np.linspace(0, 3, 1000).reshape(1000, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdD6Mv1rzXM"
      },
      "source": [
        "### Task 8\n",
        "**Ridge regression is one such form of regularized regression**.\n",
        "\n",
        "Create a [ridge linear model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) with `alpha=1` and Stochastic Average Gradient <br> \n",
        "descent (`solver=\"sag\"`) solver to `X` and `y`. <br>\n",
        "Fit (`.fit()`) your model to `X` and `y`.<br>\n",
        "Check which value it predicts (`.predict()`) for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BToG8SUwrzXM"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VALJg_UBHQr"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5M1mEPGfrzXM"
      },
      "outputs": [],
      "source": [
        "# ridge_reg_sag = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRMGJ7M9BKKb"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_YgMtxrzXN"
      },
      "source": [
        "Now we fit a linear model (left plot) and a degree 15 polynomial model <br> \n",
        "(right plot) with ridge regression for different values of alpha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXJOazhMrzXN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Most of the following function should loook familiar from other fit and plot\n",
        "# loops in this hands-on\n",
        "def plot_model(model_class, polynomial, alphas, **model_kargs):\n",
        "    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n",
        "        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n",
        "        if polynomial:\n",
        "            model = Pipeline([\n",
        "                    (\"poly_features\", PolynomialFeatures(degree=15, include_bias=False)),\n",
        "                    (\"std_scaler\", StandardScaler()),\n",
        "                    (\"regul_reg\", model),\n",
        "                ])\n",
        "        model.fit(X, y)\n",
        "        y_new_regul = model.predict(X_new)\n",
        "        lw = 2 if alpha > 0 else 1\n",
        "        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n",
        "    plt.plot(X, y, \"b.\", linewidth=3)\n",
        "    plt.legend(loc=\"upper left\", fontsize=15)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.axis([0, 3, 0, 10])\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCDx4VkCJeLz"
      },
      "source": [
        "### Task 9\n",
        "- Describe the effect of the regularization parameter alpha in the plot <br> on the right hand side.\n",
        "- Which curve (blue, green, red) would you expect to have the smallest <br>\n",
        "training loss?\n",
        "- Which curve would you expect to generalize the best to new data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK4kWJQqLHck"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS1HfH6aLIyA"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 7 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r9Ne4pkLIgi"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also add regularization penalties as arguments to other sklearn <br>\n",
        "model classes. **Below we'll show a couple of other ways to add** <br>\n",
        "**regularization to your model training**.\n",
        "\n",
        "L2 = Ridge regularization = square value regularization<br>\n",
        "L1 = Lasso regularization = absolute value regularization"
      ],
      "metadata": {
        "id": "m_xo-LzfuWIm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYjBx6ourzXN"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_reg.fit(X, y.ravel())\n",
        "sgd_reg.predict([[1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IZ_L8urzXN"
      },
      "source": [
        "The same, but with Lasso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQSD89h_rzXN"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(121)\n",
        "plot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.subplot(122)\n",
        "plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), random_state=42)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujSNU_ferzXO"
      },
      "source": [
        "### Task 10\n",
        "create a Lasso linear model with `alpha=1` and assign it to <br> `lasso_reg`. Fit the model to X and y using `.fit()`. Check which value <br> \n",
        "it predicts for `X=[[1.5]]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlgquD2rzXO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_N97bVrEP1Q"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcInIrjgrzXO"
      },
      "outputs": [],
      "source": [
        "# lasso_reg = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxjZzo2YEUEx"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXbwrujrzXP"
      },
      "source": [
        "## Early stopping example\n",
        "\n",
        "**Sometimes our final model at the end of training is not as good as the** <br>\n",
        "**best model from the whole training procedure**. This could be true if, <br>\n",
        "for example, your model overfits to the training data. **Below we'll show** <br>\n",
        "**how you can save the best model from all of training with sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSNv0btDrzXP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "m = 100\n",
        "X = 6 * np.random.rand(m, 1) - 3\n",
        "y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hYP8Ti5rzXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "poly_scaler = Pipeline([\n",
        "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
        "        (\"std_scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
        "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
        "\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    if val_error < minimum_val_error: # Check if this is our best model\n",
        "        minimum_val_error = val_error # Overwrite the minimum error\n",
        "        best_epoch = epoch # Record the best epoch\n",
        "\n",
        "        # Notice how we save a deep copy of the model instead of just writing\n",
        "        # best_model = sgd_reg. This is because assigning a variable to a\n",
        "        # different variable can sometimes cause python to treat them as one\n",
        "        # object; meaning changes made to one would affect the other.\n",
        "        best_model = deepcopy(sgd_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also do something similar when searching for good hyper- <br> parameters (e.g. number of epochs) by **saving our loss history instead** <br>\n",
        "**of saving the best model**."
      ],
      "metadata": {
        "id": "lECmhOKwyyj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEiOzmTjrzXP"
      },
      "outputs": [],
      "source": [
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,\n",
        "                       penalty=None, learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "n_epochs = 500\n",
        "train_errors, val_errors = [], []\n",
        "for epoch in range(n_epochs):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)\n",
        "    y_train_predict = sgd_reg.predict(X_train_poly_scaled)\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
        "\n",
        "best_epoch = np.argmin(val_errors)\n",
        "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
        "\n",
        "plt.annotate('Best model',\n",
        "             xy=(best_epoch, best_val_rmse),\n",
        "             xytext=(best_epoch, best_val_rmse + 1),\n",
        "             ha=\"center\",\n",
        "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "             fontsize=16,\n",
        "            )\n",
        "\n",
        "best_val_rmse -= 0.03  # just to make the graph look better\n",
        "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
        "plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"Validation set\")\n",
        "plt.plot(np.sqrt(train_errors), \"r--\", linewidth=2, label=\"Training set\")\n",
        "plt.legend(loc=\"upper right\", fontsize=14)\n",
        "plt.xlabel(\"Epoch\", fontsize=14)\n",
        "plt.ylabel(\"RMSE\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLty71JWrzXQ"
      },
      "source": [
        "### Task 11\n",
        "Suppose you use Batch Gradient Descent and you plot the validation <br> error at every epoch. If you notice that the validation error <br>\n",
        "consistently goes up, what is likely going on? <br>\n",
        "What would be options for fixing this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBxdA9gcEjYo"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiQ8itC_LsLC"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 11 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXsEtJj3Egno"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okk_0qAcrzXQ"
      },
      "source": [
        "### Task 12\n",
        "We want to find at which exact epoch our model is the best. <br>\n",
        "For that we go through 1000 \"epochs\".\n",
        "\n",
        "We have already prepared part of the code for this task. You only have <br> \n",
        "to fill in the last part which should do the following:\n",
        "\n",
        "- if the validation error (of the current epoch) is smaller than the <br> smallest validation error until now (`minimum_val_error`), then:\n",
        "    - set the `minimum_val_error` to be the current validation error\n",
        "    - set the `best_epoch` to be the current epoch\n",
        "    - set the `best_model` to be the current model (`clone(sgd_reg)`)\n",
        "- else: next epoch (you can also just leave out `else`)\n",
        "\n",
        "**You can check the early stop example above if you're unsure how to do <br>\n",
        "this.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGsvUE8JrzXQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n",
        "sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None,\n",
        "                       learning_rate=\"constant\", eta0=0.0005, random_state=42)\n",
        "\n",
        "minimum_val_error = float(\"inf\")\n",
        "best_epoch = None\n",
        "best_model = None\n",
        "for epoch in range(1000):\n",
        "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
        "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
        "    val_error = mean_squared_error(y_val, y_val_predict)\n",
        "    # ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below\n",
        "\n",
        "    # ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this\n",
        "    \n",
        "print(\"Best epoch: \", best_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drZACu6vMYVz"
      },
      "source": [
        "Task 12.5 bonus question: Why do we need `clone(sgd_reg)` here and not <br> \n",
        "just `sgd_reg`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHpMfph7M0-c"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImSMkULxMgP9"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 12.5 (bonus) answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO-6REp_M3ul"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqqN4XQ5rzXQ"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "Below we'll see an example of a model that's sometimes referred to as a <br>\n",
        "**logistic regression classifier**. The model essentially tries to map <br>\n",
        "multiple variables to the log-odds of a particular class being selected <br>\n",
        "in such a way that the log-odds is a linear combination of the <br>\n",
        "variables.\n",
        "\n",
        "Note: logistic regression models include:\n",
        "- maximum entropy models (the multi-class case)\n",
        "- binomial logistic regression models (the binary case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgmxSMiprzXR"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(-10, 10, 100)\n",
        "sig = 1 / (1 + np.exp(-t))\n",
        "plt.figure(figsize=(9, 3))\n",
        "plt.plot([-10, 10], [0, 0], \"k-\")\n",
        "plt.plot([-10, 10], [0.5, 0.5], \"k:\")\n",
        "plt.plot([-10, 10], [1, 1], \"k:\")\n",
        "plt.plot([0, 0], [-1.1, 1.1], \"k-\")\n",
        "plt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\n",
        "plt.xlabel(\"t\")\n",
        "plt.legend(loc=\"upper left\", fontsize=20)\n",
        "plt.axis([-10, 10, -0.1, 1.1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q175BB8YM_4e"
      },
      "source": [
        "From here on we will work with the [Iris Flower Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
        "\n",
        "First, let's import that dataset from sklearn's collection of datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCGkSSy2rzXR"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NYUu3OerzXR"
      },
      "outputs": [],
      "source": [
        "print(iris.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGHNbX4OHWk"
      },
      "source": [
        "Let's only use one feature: The petal width\n",
        "\n",
        "`iris[\"data\"][:, 3:]` is getting the values for petal width.\n",
        "\n",
        "We need to convert our labels to integers (they're strings right now)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJnA7NulrzXR"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, 3:]  # petal width\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a logistic regression model and fit it to our data."
      ],
      "metadata": {
        "id": "Lo6fNNuO3yyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUEwUjZerzXR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
        "log_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbwNtyS6rzXS"
      },
      "outputs": [],
      "source": [
        "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
        "y_proba = log_reg.predict_proba(X_new)\n",
        "decision_boundary = X_new[y_proba[:, 1] >= 0.5][0]\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.plot(X[y==0], y[y==0], \"bs\")\n",
        "plt.plot(X[y==1], y[y==1], \"g^\")\n",
        "plt.plot([decision_boundary, decision_boundary], [-1, 2], \"k:\", linewidth=2)\n",
        "plt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris virginica\")\n",
        "plt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris virginica\")\n",
        "plt.text(decision_boundary+0.02, 0.15, \"Decision  boundary\", fontsize=14, color=\"k\", ha=\"center\")\n",
        "plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')\n",
        "plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')\n",
        "plt.xlabel(\"Petal width (cm)\", fontsize=14)\n",
        "plt.ylabel(\"Probability\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 3, -0.02, 1.02])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNS8yXYRrzXS"
      },
      "outputs": [],
      "source": [
        "decision_boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnfuLBlOWj_"
      },
      "source": [
        "The decision boundary is at Petal width = 1.66cm. <br>\n",
        "Let's see what we get if we predict the class of an instance slightly <br> \n",
        "to the left (1.5) and one slightly to the right (1.7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy-BKAq2NZAK"
      },
      "outputs": [],
      "source": [
        "# if you are motivated: Try to understand this output\n",
        "log_reg.predict_proba([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5I1RefjrzXS"
      },
      "outputs": [],
      "source": [
        "log_reg.predict([[1.7], [1.5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MobMBsLOvGb"
      },
      "source": [
        "### Task 13\n",
        "What is the class prediction (iris virginica or not iris virginica?) <br> \n",
        "for petal width = 1.7 and what is the prediction for petal width = 1.5?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ola1BG00PINb"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxOueRiLPI6p"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 13 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abO95ce6PKrO"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKGQwlMPRM5"
      },
      "source": [
        "Now let's use two features for predicting the class: <br>\n",
        "The petal length and the petal width. <br>\n",
        "We also use all classes this time and not just two like above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tp3V7ukBnYP"
      },
      "source": [
        "## Extension to 2 dimensions\n",
        "There are no tasks for this part. You can try to understand what <br>\n",
        "happens or skip if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo0ZrnDKrzXS"
      },
      "outputs": [],
      "source": [
        "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
        "y = iris[\"target\"]\n",
        "\n",
        "softmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10, random_state=42)\n",
        "softmax_reg.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "augxRMiGrzXS"
      },
      "outputs": [],
      "source": [
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "\n",
        "\n",
        "y_proba = softmax_reg.predict_proba(X_new)\n",
        "y_predict = softmax_reg.predict(X_new)\n",
        "\n",
        "zz1 = y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"center left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEL3UyOgrzXT"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict([[5, 2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYq9mMx1rzXT"
      },
      "outputs": [],
      "source": [
        "softmax_reg.predict_proba([[5, 2]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn-Qmqd9BnYQ"
      },
      "source": [
        "## General Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBGj1xiqrzXT"
      },
      "source": [
        "### Task 14\n",
        "Can Gradient Descent get stuck in a local minimum when training a <br> Logistic Regression model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmNY_mlQJ_y"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpat0HnMrzXT"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 14 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW6VxC40QMZo"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7liesyzIrzXT"
      },
      "source": [
        "### Task 15\n",
        "Which Gradient Descent algorithm (among those we discussed) will reach <br> \n",
        "the vicinity of the optimal solution the fastest? Which will actually <br> \n",
        "converge? How can you make the others converge as well?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGyb_lSkQPZa"
      },
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRt4Sv5JrzXT"
      },
      "source": [
        "### Double Click Here\n",
        "\n",
        "Task 15 answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC9eg3tOQOD2"
      },
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}