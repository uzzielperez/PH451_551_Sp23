{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtz5W9aFvIESXdzQhiiFNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UAPH451551/PH451_551_Sp23/blob/main/Exercises/08B_rnns_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs, Attention, and Transformers"
      ],
      "metadata": {
        "id": "bPYku5S2bZf4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_5rT9hCW_Yu"
      },
      "source": [
        "Due date: 2023-04-07\n",
        "\n",
        "File name convention: For group 42 and memebers Richard Stallman and Linus <br>\n",
        "Torvalds it would be <br>\n",
        "\"08_Exercise8_group42_Stallman_Torvalds.pdf\".\n",
        "\n",
        "Submission via blackboard (UA).\n",
        "\n",
        "Feel free to answer free text questions in text cells using markdown and <br>\n",
        "possibly $\\LaTeX{}$ if you want to.\n",
        "\n",
        "**You don't have to understand every line of code here and it is not intended <br> \n",
        "for you to try to understand every line of code.   <br>\n",
        "Big blocks of code are usually meant to just be clicked through.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "metadata": {
        "id": "YWgUpTD9Rxac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preparing the Dataset"
      ],
      "metadata": {
        "id": "Qme7akjZPzP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_time_series(batch_size, n_steps):\n",
        "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
        "    time = np.linspace(0, 1, n_steps)\n",
        "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  #   wave 1\n",
        "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
        "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + noise\n",
        "    return series[..., np.newaxis].astype(np.float32)"
      ],
      "metadata": {
        "id": "qLie-1cRsPAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "n_steps = 50\n",
        "series = generate_time_series(10000, n_steps + 1)\n",
        "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
        "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
        "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
      ],
      "metadata": {
        "id": "jzdTaLissSv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "dqiEg16esU4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(series, y=None, y_pred=None, x_label=\"$t$\", y_label=\"$x(t)$\", legend=True):\n",
        "    plt.plot(series, \".-\")\n",
        "    if y is not None:\n",
        "        plt.plot(n_steps, y, \"bo\", label=\"Target\")\n",
        "    if y_pred is not None:\n",
        "        plt.plot(n_steps, y_pred, \"rx\", markersize=10, label=\"Prediction\")\n",
        "    plt.grid(True)\n",
        "    if x_label:\n",
        "        plt.xlabel(x_label, fontsize=16)\n",
        "    if y_label:\n",
        "        plt.ylabel(y_label, fontsize=16, rotation=0)\n",
        "    plt.hlines(0, 0, 100, linewidth=1)\n",
        "    plt.axis([0, n_steps + 1, -1, 1])\n",
        "    if legend and (y or y_pred):\n",
        "        plt.legend(fontsize=14, loc=\"upper left\")\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(12, 4))\n",
        "for col in range(3):\n",
        "    plt.sca(axes[col])\n",
        "    plot_series(X_valid[col, :, 0], y_valid[col, 0],\n",
        "                y_label=(\"$x(t)$\" if col==0 else None),\n",
        "                legend=(col == 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UBMBEznYsYkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "Nnn8DZ8KQ-eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Simple Recurrent Neural Network RNN\n",
        "\n",
        "The simplest form of **RNN** one can build **takes an output and passes it into the** <br>\n",
        "**next input**. The idea is that, if your data has some form of **sequential** property <br>\n",
        "such as being elements of a **time series** or a sentence or even generally related <br>\n",
        "(not explicitly sequential) values, you should attempt to **inform your model via** <br>\n",
        "**explicit neural connections** that those values are linked. By passing the output <br>\n",
        "of one neuron to the next neuron in a sequence, you can explicitly inform your <br>\n",
        "model of the relationship of an input to its neighboring inputs."
      ],
      "metadata": {
        "id": "tC81tARAS0yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers"
      ],
      "metadata": {
        "id": "EVRq9CisVbeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Simple RNN\n",
        "\n",
        "Build  a simple RNN using the `SimpleRNN` layer. Your model should be a <br>\n",
        "Sequential model called `simplernn` of the following form:<br>\n",
        "1) `SimpleRNN` with 16 units, `return_sequences=True`, and `input_shape=[None,1]`<br>\n",
        "2) `SimpleRNN` layer with 16 units and `return_sequences=False`<br>\n",
        "3) Output `Dense` layer with 1 output dimension with a `'linear'` activation.\n",
        "\n",
        "Compile the model with `'mse'` loss and the `'adam'` optimizer.<br>\n",
        "Train the model on `X_train` and `y_train`."
      ],
      "metadata": {
        "id": "bu-O5kL7w7PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "qhl_mpxPyMIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simplernn = keras.models.Sequential([])\n",
        "# simplernn.compile()\n",
        "# simplernn.fit()"
      ],
      "metadata": {
        "id": "f_IY8OsL_a2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "DsqLAiqTyTfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplernn.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "axqofEdK0KVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curves(loss, val_loss):\n",
        "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
        "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
        "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
        "    plt.axis([1, 10, 0.002, 0.01])\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "NLtV-xKt0viV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FfzfB3Kp0mcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = simplernn.predict(X_valid)\n",
        "plot_series(X_valid[0, :, 0], y_valid[0, 0], y_pred[0, 0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G4QBEMeb0qaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)"
      ],
      "metadata": {
        "id": "J0qYfkYPIZRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(43) # not 42, as it would give the first series in the train set\n",
        "\n",
        "new_series = generate_time_series(1, n_steps + 10)\n",
        "X_new, Y_new = new_series[:, :n_steps], new_series[:, n_steps:]"
      ],
      "metadata": {
        "id": "T-MHM_8GkYIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplernn.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "5CYO4eb_74sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_new.shape"
      ],
      "metadata": {
        "id": "TqrMpy2H78RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_multiple_forecasts(X, Y, Y_pred):\n",
        "    n_steps = X.shape[1]\n",
        "    ahead = Y.shape[1]\n",
        "    plot_series(X[0, :, 0])\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y[0, :, 0], \"bo-\", label=\"Actual\")\n",
        "    plt.plot(np.arange(n_steps, n_steps + ahead), Y_pred[0, :, 0], \"rx-\", label=\"Forecast\", markersize=10)\n",
        "    plt.axis([0, n_steps + ahead, -1, 1])\n",
        "    plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "_5m0f60G77mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XxxMhNLb7ztQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Gated Recurrent Unit (GRU)\n",
        "\n",
        "GRU models are **actually very new** in the timeline of RNNs having been first <br>\n",
        "**proposed in 2014** by Kyunghyun Cho et al. The main feature that this adds to the <br>\n",
        "RNN architechture is that it replaces the simple densely connected unit with a <br>\n",
        "new unit that adds a **\"forget gate\"**. You might remember the **characteristic** <br>\n",
        "**equation for a standard neuron** is $a_i=\\sigma(w_ix_i + b_l)$. For the GRU you add <br>\n",
        "a term which depends on a new weight we can call u which is applied to the <br>\n",
        "output of the previous neuron: $a_i=\\sigma(w_ix_i + u_ih_{i-1} + b_l)$ where h is the final <br>\n",
        "output of the previous neuron. Additionally, it takes that initial activation and <br>\n",
        "applies two steps:<br>\n",
        "1) $\\hat h_i=\\phi(w_{2i}x_i+u_{2i}(a_i\\odot h_{i-1}) + b_{2l})$ where $\\phi$ is tanh and $w_2$ is a second <br>\n",
        "neuron weight meaning that the simplest **GRU will have 4 weights and 2 biases**.<br>\n",
        "2) $h_i=(1-a_i)\\odot h_{i-1}+a_i\\odot \\hat h_i$)\n",
        "\n",
        "If we consider what those equations mean then you can probably work out that <br>\n",
        "**(1), our forget gate**, is checking how correlated our activation is with our <br> \n",
        "previous neuron's activation. Then, **if our output is highly correlated**, our <br> \n",
        "final activation **(2) will have a large contribution from our previous neuron** <br>\n",
        "and if uncorrelated, it will have a small contribution.\n",
        "\n",
        "The modern standard implementation of GRU also now includes a \"reset gate\" and <br>\n",
        "an \"update gate\" which you can read about further [here](https://en.wikipedia.org/wiki/Gated_recurrent_unit)."
      ],
      "metadata": {
        "id": "RLnjnt_gR2Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Simple GRU Model\n",
        "\n",
        "Build an identical model to task 1 but substituting `GRU` layers instead of <br>\n",
        "`SimpleRNN` layers and instead call the model `simplegru`.\n",
        "\n",
        "Train the model on `X_train` and `y_train`."
      ],
      "metadata": {
        "id": "gdmxr2joyzAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "iIbbvXR7zopr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simplegru = keras.models.Sequential([])\n",
        "# simplegru.compile()\n",
        "# simplegru.fit()"
      ],
      "metadata": {
        "id": "oEQ2_LBN_Gtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "a89v3Z9izsgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplegru.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "AGknk6XItDz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplegru.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "N70bpTybf7GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BeMdII5jf_qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTM's were among the state of the art for language processing tasks for quite <br>\n",
        "some time. Interestingly though, they were **first proposed in 1995** by Hochreiter <br>\n",
        "and Schmidhuber almost **20 years before GRU**. In fact, LSTM models **outperform** <br>\n",
        "**GRU models** on many tasks and by many metrics. However, **not without costs**. LSTM <br>\n",
        "cells require **more storage space** and are **slower** than GRU cells. This is due to <br>\n",
        "the fact that they have an additional **\"long-term memory\"** value which they can <br>\n",
        "use to preserve information even between iterations. Whereas GRU will only <br>\n",
        "\"remember\" information passed by adjacent neurons, **LSTM will remember** that and <br>\n",
        "also information passed by **previous batches or iterations of data**."
      ],
      "metadata": {
        "id": "gifx3hG1XQ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplelstm = keras.models.Sequential([\n",
        "    layers.LSTM(16, return_sequences=True, input_shape=[None,1]),\n",
        "    layers.LSTM(16),\n",
        "    layers.Dense(1, activation='linear'),\n",
        "])"
      ],
      "metadata": {
        "id": "g0zFZRUUfxLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplelstm.compile(loss=\"mse\", optimizer=\"adam\")\n",
        "history = simplelstm.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20)"
      ],
      "metadata": {
        "id": "8N_PTEVrtPSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplelstm.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "FOD8YHl8tPSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = simplelstm.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "HwZP2dXutPSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CHKhpfVAgPO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Analyzing Model Performance\n",
        "\n",
        "Question: Compare the behavior of LSTM to GRU and simple RNN. Which region of <br>\n",
        "future forecasting is most different? Describe characteristics about the <br>\n",
        "various models that would lead to that difference."
      ],
      "metadata": {
        "id": "TbkvrUiwz698"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ],
      "metadata": {
        "id": "8FQZsm8u0sR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3) answer:"
      ],
      "metadata": {
        "id": "l-mqOqLq0s7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ],
      "metadata": {
        "id": "DSyuEsHw0sR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Luong Attention Mechanism\n",
        "\n",
        "Attention is an operation built around applying a **dot product** across **1) outputs** <br>\n",
        "**from the head of a model** which has \"encoded\" the data by transforming with <br>\n",
        "weights and biases and **2) hidden states (weights) from the tail of a model** <br>\n",
        "which is attempting to \"decode\" the data and turn it into your desired output.\n",
        "\n",
        "That is a long sentence but what it breaks down to is that you're **reweighting** <br>\n",
        "**data** part way through a model so that the back half of your model knows what's <br>\n",
        "most important from what the first half did. When Luong, Pham and Manning <br>\n",
        "published [this paper](https://arxiv.org/abs/1508.04025) in 2015, they achieved \n",
        "**state of the art results** and laid the <br>\n",
        "foundation for the **Third Wave of AI** driven largely by attention-based <br>\n",
        "Transformer models."
      ],
      "metadata": {
        "id": "DoBM0jI8ZwrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "ReVDqdmED8Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=X_train.shape[1:])\n",
        "x, f_h, f_c = layers.LSTM(16, return_state=True, return_sequences=True)(inputs)\n",
        "attention = layers.Activation('softmax')(f_h)\n",
        "context = layers.dot([attention, f_h], axes=[1,1])\n",
        "f_h = layers.add([f_h, context])\n",
        "y = layers.LSTM(16)(x, initial_state=[f_h, f_c])\n",
        "outputs = layers.Dense(1, activation='linear')(y)\n",
        "\n",
        "attlstm = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "QMilkktLgUAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Understanding Luong Attention\n",
        "\n",
        "Here `f_h` is the hidden state output from an encoder and `f_c` is the cell <br>\n",
        "state from an encoder. Describe, to the best your ability, what is happening <br>\n",
        "between the extraction of the hidden state from the encoder and passing the <br>\n",
        "hidden state as the initial state of the decoder (between lines 2-6)."
      ],
      "metadata": {
        "id": "QAQaRKjY05D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ],
      "metadata": {
        "id": "QeeQaY-g2nuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4) answer:"
      ],
      "metadata": {
        "id": "WquNeYjy2nuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ],
      "metadata": {
        "id": "uHVpLREr2nuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=10e-4)\n",
        "attlstm.compile(loss=\"mse\", optimizer=optimizer)\n",
        "history1 = attlstm.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20)"
      ],
      "metadata": {
        "id": "7TfHnFHI2ihf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attlstm.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "t0H2J_Ph2ihh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = attlstm.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "nfe39B0mhBOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HR2C8F7fhBOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Transformer Model\n",
        "\n",
        "The transformer architecture was first proposed in a paper called <br>\n",
        "[Attention is All You Need (Vaswani et. al 2017](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "The characteristic operation of the transformer is the **self-attention** <br>\n",
        "operation. In essence, it is using **dot product** operations along different axes <br>\n",
        "of the data to amplify the importance of relevant data. You can think of this <br>\n",
        "as effectively building **a model that reweights its inputs** to pick out the most <br>\n",
        "important and relevant inputs. Then it does typical mathematical operations of <br>\n",
        "**densely connected layers** on those reweighted inputs.\n",
        "\n",
        "Researchers figured out several years before this that dot-product attention <br>\n",
        "could be used in combination with other forms of models to improve performance <br>\n",
        "by reweighting input data but the **Attention is All You Need** paper showed that, <br>\n",
        "as the name would imply, you can get even **better performance** by eliminating all <br>\n",
        "other characteristic layers and instead **using only attention and dense layers**."
      ],
      "metadata": {
        "id": "mu6pVFsZWG89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "BALMDUKmzW46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Definining model dimensions\n",
        "\n",
        "Define two variables: <br>\n",
        "1) `max_len` should be equal to the size of the dimension from X_train that <br>\n",
        "describes the length of the time series. **Hint: see above plots**<br>\n",
        "2) `embedding_dim` should be equal to the fourth root (**(1/4)) of the total <br>\n",
        "possible data points in the input and output"
      ],
      "metadata": {
        "id": "j1J2HsBJ2zwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "EHhIpWbF57L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len =\n",
        "# embedding_dim =\n",
        "# print(max_len)\n",
        "# print(embedding_dim)"
      ],
      "metadata": {
        "id": "iF2tRgjxdI8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "m-Fphtin5wsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's typical with transformers to embed our inputs into a higher dimensional <br>\n",
        "space. For discrete data such as integers or words, it's typical to use an  <br>\n",
        "embedding layer from the keras toolkit. However, for continuous data, it's more <br>\n",
        "common to embed using linear or non-linear transformations."
      ],
      "metadata": {
        "id": "FnMI23J6lVGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearEmbedding(layers.Layer):\n",
        "  def __init__(self, dim, dtype=tf.float32, **kwargs):\n",
        "    super().__init__(dtype=dtype, **kwargs)\n",
        "    self.dense = layers.Dense(dim, activation='linear')\n",
        "  def call(self, inputs):\n",
        "    inputs = tf.expand_dims(inputs, axis=-1)\n",
        "    return self.dense(inputs)"
      ],
      "metadata": {
        "id": "uGMFrPX8QgST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another interesting feature of transformers is that they are not explicitly <br>\n",
        "aware of the order of data in a relative sequence, especially after embedding. <br>\n",
        "We therefore can inform it of positions by, for example, adding a small amount <br>\n",
        "related to its position in the tensor by some interpretable form like periodic <br>\n",
        "functions."
      ],
      "metadata": {
        "id": "QJHKxTVrlyWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, max_dims, alpha=1, dtype=tf.float32, **kwargs):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
        "        p, i = np.meshgrid(np.arange(max_len), np.arange(max_dims // 2))\n",
        "        pos_emb = np.empty((1, max_len, max_dims))\n",
        "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
        "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
        "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))*alpha\n",
        "    def call(self, inputs):\n",
        "        shape = tf.shape(inputs)\n",
        "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
      ],
      "metadata": {
        "id": "ufzj9vPHVXRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32"
      ],
      "metadata": {
        "id": "tkdrAjm6Vta-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two colab cells for simple transformers were derived from [this](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/)<br>\n",
        "article by Stefania Cristina on machinelearningmastery.com"
      ],
      "metadata": {
        "id": "B1RCSfU9310V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
        "K = keras.backend\n",
        "\n",
        "class DotProductAttention(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        " \n",
        "    def call(self, queries, keys, values, d_k, mask=None):\n",
        "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
        "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
        " \n",
        "        # Apply mask to the attention scores\n",
        "        if mask is not None:\n",
        "            scores += -1e9 * mask\n",
        " \n",
        "        # Computing the weights by a softmax operation\n",
        "        weights = K.softmax(scores)\n",
        " \n",
        "        # Computing the attention by a weighted sum of the value vectors\n",
        "        return matmul(weights, values)"
      ],
      "metadata": {
        "id": "MxzLufKmV4Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras/\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
        "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
        "        self.heads = h  # Number of attention heads to use\n",
        "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
        "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
        "        self.d_model = d_model  # Dimensionality of the model\n",
        "        self.W_q = layers.Dense(d_k)  # Learned projection matrix for the queries\n",
        "        self.W_k = layers.Dense(d_k)  # Learned projection matrix for the keys\n",
        "        self.W_v = layers.Dense(d_v)  # Learned projection matrix for the values\n",
        "        self.W_o = layers.Dense(d_model)  # Learned projection matrix for the multi-head output\n",
        " \n",
        "    def reshape_tensor(self, x, heads, flag):\n",
        "        if flag:\n",
        "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "        else:\n",
        "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
        "            x = transpose(x, perm=(0, 2, 1, 3))\n",
        "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
        "        return x\n",
        " \n",
        "    def call(self, inputs, mask=None):\n",
        "        queries = keys = values = inputs\n",
        "        # Rearrange the queries to be able to compute all heads in parallel\n",
        "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the keys to be able to compute all heads in parallel\n",
        "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange the values to be able to compute all heads in parallel\n",
        "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
        "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask=mask)\n",
        "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
        " \n",
        "        # Rearrange back the output into concatenated form\n",
        "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
        " \n",
        "        # Apply one final linear projection to the output to generate the multi-head attention\n",
        "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
        "        return self.W_o(output)"
      ],
      "metadata": {
        "id": "A2DTZqHDWCsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_block(inputs, n_heads, max_len, ff_dim, dropout_rate):\n",
        "    # Multi-head attention\n",
        "    attention = MultiHeadAttention(h=n_heads, d_k=max_len, \n",
        "                                   d_v=max_len, d_model=ff_dim)(inputs)\n",
        "    attention = layers.Dropout(dropout_rate)(attention)\n",
        "    add1 = layers.Add()([inputs, attention])\n",
        "    norm1 = layers.LayerNormalization(epsilon=1e-6)(add1)\n",
        "\n",
        "    # Feedforward network\n",
        "    ff = layers.Dense(units=ff_dim, activation=\"relu\")(norm1)\n",
        "    ff = layers.Dense(units=embedding_dim)(ff)\n",
        "    ff = layers.Dropout(dropout_rate)(ff)\n",
        "    add2 = layers.Add()([norm1, ff])\n",
        "    norm2 = layers.LayerNormalization(epsilon=1e-6)(add2)\n",
        "\n",
        "    return norm2"
      ],
      "metadata": {
        "id": "GXBgqBkfVb-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_heads = 1\n",
        "ff_dim = embedding_dim\n",
        "dropout_rate = 0.0 # This should be >0 but <1 for more complex data sets\n",
        "num_blocks = 1\n",
        "\n",
        "inputs = layers.Input(shape=(max_len,))\n",
        "embedding_layer = LinearEmbedding(embedding_dim)\n",
        "encoder_embeddings = embedding_layer(inputs)\n",
        "positional_encoding = PositionalEncoding(max_len, max_dims=embedding_dim, alpha=.5)\n",
        "encoder_in = positional_encoding(encoder_embeddings)\n",
        "\n",
        "blocks = []\n",
        "for i in range(num_blocks):\n",
        "    blocks.append(transformer_block(encoder_in, n_heads, max_len, ff_dim, dropout_rate))\n",
        "transfout = layers.Concatenate(axis=-1)([block for block in blocks])"
      ],
      "metadata": {
        "id": "BDezv_v9VfkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pooled = layers.GlobalAveragePooling1D()(transfout)"
      ],
      "metadata": {
        "id": "3t_jWXirVhtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Global Average Pooling\n",
        "\n",
        "Here we use a `GlobalAveragePooling1D` layer instead of simply adding, <br>\n",
        "multiplying or flattening. Explain why we use this instead of the previously <br>\n",
        "mentioned operations. Feel free to use online sources."
      ],
      "metadata": {
        "id": "SdeueqfJ5Eyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your answer goes below"
      ],
      "metadata": {
        "id": "UVVK7fPm5qzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6) answer:"
      ],
      "metadata": {
        "id": "kTU-yEY95qzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your answer goes above this"
      ],
      "metadata": {
        "id": "UVPuUZiN5qzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = layers.Dense(1, activation='linear')(pooled)"
      ],
      "metadata": {
        "id": "GCWkFbLiVkd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note: Transformers are often much larger in dimensionality than RNNs so we may** <br>\n",
        "**need to use different training procedure like schedules for best results.**"
      ],
      "metadata": {
        "id": "qPz2P5Ab6LvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = keras.Model(inputs=inputs, outputs=outputs)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=10e-4)\n",
        "transformer.compile(optimizer=optimizer, loss=\"mse\")"
      ],
      "metadata": {
        "id": "-cBRAmg4Vme5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20)"
      ],
      "metadata": {
        "id": "FKqQ39TWVqEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
        "transformer.compile(optimizer=optimizer, loss=\"mse\")\n",
        "transformer.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)"
      ],
      "metadata": {
        "id": "nj8PIIK0TOH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_new\n",
        "for step_ahead in range(10):\n",
        "    y_pred_one = transformer.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
        "    X = np.concatenate([X, y_pred_one], axis=1)\n",
        "\n",
        "y_pred_new = X[:, n_steps:]"
      ],
      "metadata": {
        "id": "LJ3zPYLAlsaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_multiple_forecasts(X_new, Y_new, y_pred_new)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pTLGchPClsaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT\n",
        "\n",
        "GPT stands for **\"generative, pre-trained transformer\"**. ChatGPT is a version of <br>\n",
        "a gpt **created by OpenAI** which made waves in late 2022 because of how well it <br>\n",
        "mimics human conversation and interaction. Below, we'll look at how to access <br>\n",
        "**ChatGPT in colab using a public API**. It is not possible to house the full model <br>\n",
        "in a typical colab notebook because the model has an astounding **175 billion** <br>\n",
        "**model parameters**. For comparison, two fully connected dense layers with 10,000 <br>\n",
        "neurons each would only have around 100 million parameters.\n",
        "\n",
        "In March 2023, OpenAI also released GPT4 which has over **1 TRILLION!** <br>\n",
        "parameters. The model extended the capabilities of previous GPT iterations to <br>\n",
        "include vision. It's so far shown incredible ability, even able to score in the <br>\n",
        "**90th percentile on the Bar exam**, the final examination to become a lawyer in <br>\n",
        "the United States."
      ],
      "metadata": {
        "id": "h823RwThqvCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "15dacVG8mmXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Create an acccount at https://platform.openai.com\n",
        "# Replace YOUR_API_KEY with your actual API key for the ChatGPT service:\n",
        "# https://platform.openai.com/account/api-keys\n",
        "openai.api_key = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "wBNp6dPKmoFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.Completion.create(\n",
        "    engine=\"davinci\", # The specific version of GPT model to use\n",
        "    prompt=\"What is dot-product attention?\", # What you feed into the model\n",
        "    max_tokens=300, # Max number of characters in output\n",
        "    n=1, # Number of responses to generate\n",
        "    stop=None, # When the output stops\n",
        "    temperature=0.0, # Controls the randomness of the output (>=0)\n",
        ")\n",
        "\n",
        "message = response.choices[0].text.strip()\n",
        "print(message)"
      ],
      "metadata": {
        "id": "0SrJomqfmu7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: ChatGPT for Education\n",
        "\n",
        "Using ChatGPT, generate an interesting piece of educational content about the <br>\n",
        "current topic of RNNs and/or Transformers. **Hint: better promt=better response**<br>"
      ],
      "metadata": {
        "id": "QPopArby65S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "aZ0Xck2m65S3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sis88Wfw7X8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "OMnhBJ6w65S3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8 (Bonus): Model performance analysis\n",
        "\n",
        "Analyze the performance of at least 3 models used in this exercise using at <br>\n",
        "least two different metrics on the test data sets or newly generated data. <br>\n",
        "Examples of metrics might be single-event prediction error, forecasting error, <br>\n",
        "or deviation-based metrics.\n",
        "\n",
        "Further, provide a brief text description (less than 500 words) explaining the <br>\n",
        "metric used and results."
      ],
      "metadata": {
        "id": "bDTuLad08Bzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ your code goes below"
      ],
      "metadata": {
        "id": "iT2iabKo8Bzo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Xpb1p238Bzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8) answer:"
      ],
      "metadata": {
        "id": "dpf6qZRQ-TTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑ your code goes above this"
      ],
      "metadata": {
        "id": "biRtG7Tr8Bzo"
      }
    }
  ]
}